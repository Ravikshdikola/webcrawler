
# ğŸ•·ï¸ #Product Link Crawler using Scrapy

This repository contains custom **Scrapy-based web crawlers** designed to extract **product page URLs** from multiple e-commerce websites. Each spider is tailored to the site's HTML structure for efficient and accurate data collection.



project overview :
mycrawler/
â”œâ”€â”€ spiders/
â”‚   â”œâ”€â”€ product_spider_virgo.py
â”‚   â”œâ”€â”€ product_spider_westside.py
â”‚   â”œâ”€â”€ product_spider_tataiq.py
â”‚   â””â”€â”€ product_spider_nyaka.py
â”œâ”€â”€ items.py
â”œâ”€â”€ pipelines.py
â”œâ”€â”€ settings.py




---

## ğŸ§© Spiders Overview

### `virgio_spider.py`
- **Target**: [Virgio.com](https://www.virgio.com/)
- **Highlights**:
  - Starts from the "Party Edit" collection
  - Extracts links to product and category pages
  - Handles pagination
  - Outputs unique product page URLs

### `westside_spider.py`
- **Target**: [Westside](https://www.westside.com/)
- **Highlights**:
  - Crawls through categories and subcategories
  - Extracts links to product pages

### `tataiq_spider.py`
- **Target**: Tata iQ product catalog (or a similar platform)
- **Highlights**:
  - Navigates category/product hierarchies
  - Handles nested product structures

### `nykaa_spider.py`
- **Target**: [Nykaa](https://www.nykaa.com/)
- **Highlights**:
  - Extracts product links from various beauty categories
  - Handles product variants and pagination

---

## ğŸ’¾ Output Format

Each spider yields a JSON-like dictionary:
```json
{
  "product_url": "https://example.com/products/product-name"
}


## for running it :
syntax : scrapy  crawl  <name described in each webcrawler file  -o  <output file where you want to save output it should be in json format>
scrapy crawl virgio_product_link_spider -o virgio_links.json
